{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e706306c",
   "metadata": {},
   "source": [
    "# Run training on Amazon Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe56cbf",
   "metadata": {},
   "source": [
    "The purpose of this notebook is a code example of how to train a text classifier for content moderation in AWS environment in a non ditributed manner for the first time, then showcase how to extend to a distributed fine-tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb7255",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa842ba",
   "metadata": {},
   "source": [
    "As I am using Sagemaker notebooks, I am already using a kernel with Pytorch installed. So, all I have to do is to install sagemaker, transformers and datasets to download the dataset from Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8ef5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.77.0\" \"transformers==4.12.3\" \"datasets[s3]==1.18.3\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2259c",
   "metadata": {},
   "source": [
    "# Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb900d",
   "metadata": {},
   "source": [
    "We need a sagemaker session. The session will handle the creation of a default bucket to store data, models, and logs.\n",
    "\n",
    "As I am working with a sagemaker notebook, I can use the .get_execution_role() function to access the role. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325644cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf511a",
   "metadata": {},
   "source": [
    "# Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a16594",
   "metadata": {},
   "source": [
    "I chose the tweets_hate_speech_detection dataset from Hugging Face hub to work on, as it is very close to a content moderation task.\n",
    "The labels for this dataset is either 0 (no hate speech) or 1 (hate speech), so it is not a multilabel text classification problem but the code stills the same for binary or multilabel classifiaction on AWS (except a little detail in the training script when dowloading the model with .from_pretrained : for a mulilabel class you should add the num_labels parameters and 2 dictionnaries mapping labels to ids and ids to labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f02e7e",
   "metadata": {},
   "source": [
    "I split the data into train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f8243f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6afee84412544c79183c335f348e579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce9af27f716440a8b2eed3d50d3c3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/881 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset tweets_hate_speech_detection/default (download: 2.96 MiB, generated: 3.04 MiB, post-processed: Unknown size, total: 6.00 MiB) to /home/ec2-user/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bd9caabbf64bda9b3e5e5eb75b2183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tweets_hate_speech_detection downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c6b6f41e91ac9113e1c032c5ecf7a49b4e1e9dc8699ded3c2d8425c9217568b2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cb9fa532b6427e9fac202f0c8f2614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'generator'=Generator(PCG64) of the transform datasets.arrow_dataset.Dataset.train_test_split couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load dataset\n",
    "dataset = load_dataset(\"tweets_hate_speech_detection\")\n",
    "# split the data into train and test\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac77a3",
   "metadata": {},
   "source": [
    "The preprocessing done here is just simple tokenization of the tweets.\n",
    "\n",
    "We do trunction and padding to the max length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e5cd66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e979e68b0d492da9dd70d37c561d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63008a7586b345aba7795fe53ba2b91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load tokenizer\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# create tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"tweet\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# tokenize train and test datasets\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94064867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename label with labels as the model is expecting the keyword\n",
    "tokenized_dataset =  tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# set dataset format for PyTorch\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cfa978",
   "metadata": {},
   "source": [
    "# Upload data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d33d4",
   "metadata": {},
   "source": [
    "Next, we upload the tokenized train data and test data to 2 different folders in the S3 bucket previously created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43c868ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3_prefix = 'samples/datasets/tweetsspeech'\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to S3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "tokenized_dataset[\"train\"].save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to S3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "tokenized_dataset[\"test\"].save_to_disk(test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe3dbc6",
   "metadata": {},
   "source": [
    "# Fine-tuning with Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc03c39",
   "metadata": {},
   "source": [
    "Now, we create the HuggingFace Estimator and we give it: \n",
    "* the fine-tuning script in the entry point parameter\n",
    "* the instance type of Sagemaker (To my knowledge, currently, there are only GPU DLCs for Training Hugging Face Models. so you should choose GPU-based instances otherwise you get value error unsupported cpu. So you can choose either p3 or g4 instances. for my case I tried to use the least expensive instance in my region which is ml.p3.2xlarge)\n",
    "* try to benefit from the spot instances to save costs using the parameter use_spot_instances\n",
    "* finally we give the needed hyperparameters for our training script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d7e8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "hyperparameters={\n",
    "    \"epochs\": 1,                            # number of training epochs\n",
    "    \"train_batch_size\": 32,                 # training batch size\n",
    "    \"eval_batch_size\":64,                   #validation batch size\n",
    "    \"model_name\":\"distilbert-base-uncased\",  # name of pretrained model\n",
    "}\n",
    "\n",
    "#Creating an estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",                 # fine-tuning script to use in training job\n",
    "    source_dir=\"./\",                 # directory where fine-tuning script is stored\n",
    "    instance_type=\"ml.p3.2xlarge\", # instance type\n",
    "#     checkpoint_s3_uri=f's3://{sess.default_bucket()}/checkpoints',\n",
    "#     use_spot_instances=True,\n",
    "#     max_wait=3600,\n",
    "#     max_run=1000,\n",
    "    instance_count=1,                       # number of instances\n",
    "    role=role,                              # IAM role used in training job to acccess AWS resources (S3)\n",
    "    transformers_version=\"4.6\",             # Transformers version\n",
    "    pytorch_version=\"1.7\",                  # PyTorch version\n",
    "    py_version=\"py36\",                      # Python version\n",
    "    hyperparameters=hyperparameters         # hyperparameters to use in training job\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcdad3e",
   "metadata": {},
   "source": [
    "We launch training with .fit function of the estimator object.\n",
    "\n",
    "I hit a quota limit as I am using a Free tier AWS account that don't have access to GPU-based instances. So I had to ask to augment my quota, but I didn't get an answer and I should wait for the support team to get back to me.\n",
    "\n",
    "But if you are using your professional company account, surely you have some ressources in your region and you won't have this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dfbe541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-02-26-13-18-39-985\n"
     ]
    },
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3.2xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1376/187206430.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_input_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_StepArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieve_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0mexperiment_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_and_get_run_experiment_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   2083\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2085\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2087\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, training_image_config, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intercept_create_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   4811\u001b[0m             \u001b[0mfunc_name\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mneeded\u001b[0m \u001b[0mintercepting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4812\u001b[0m         \"\"\"\n\u001b[0;32m-> 4813\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4815\u001b[0m     def _create_inference_recommendations_job_request(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intercept_create_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m                 )\n\u001b[1;32m    529\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3.2xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit."
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({\"train\": training_input_path, \"test\": test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51df4c0b",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "### Can you extend your example from 1) to a distributed setup?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717838c",
   "metadata": {},
   "source": [
    "Sagemaker provides two strategies for distributed training:\n",
    "* data parallelism\n",
    "* model parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32dc727",
   "metadata": {},
   "source": [
    "#### Data parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8ad61",
   "metadata": {},
   "source": [
    "If you want to do data parallelism, ignore the two previous cells and run the next one instead.\n",
    "As we are using the Trainer API in our fineuning script, we only need to define the distribution parameter in the Hugging Face Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf721d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "hyperparameters={\n",
    "    \"epochs\": 1,                            # number of training epochs\n",
    "    \"train_batch_size\": 32,                 # training batch size\n",
    "    \"eval_batch_size\":64,                   #validation batch size\n",
    "    \"model_name\":\"distilbert-base-uncased\",  # name of pretrained model\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed data parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "#Creating an estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",                 # fine-tuning script to use in training job\n",
    "    source_dir=\"./\",                 # directory where fine-tuning script is stored\n",
    "    instance_type=\"ml.p3dn.24xlarge\", # instance type\n",
    "    instance_count=2,                       # number of instances\n",
    "    role=role,                              # IAM role used in training job to acccess AWS resources (S3)\n",
    "    transformers_version=\"4.6\",             # Transformers version\n",
    "    pytorch_version=\"1.7\",                  # PyTorch version\n",
    "    py_version=\"py36\",                      # Python version\n",
    "    hyperparameters=hyperparameters         # hyperparameters to use in training job\n",
    "    distribution = distribution\n",
    ")\n",
    "\n",
    "huggingface_estimator.fit({\"train\": training_input_path, \"test\": test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f311b2",
   "metadata": {},
   "source": [
    "#### Model parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2229cc",
   "metadata": {},
   "source": [
    "To enable model-parallelism, a little change has to be done in the training script.\n",
    "Extend the Trainer API to a the SageMakerTrainer to use the model parallelism library by using these imports in the training script :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4567109",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers.sagemaker import SageMakerTrainingArguments as TrainingArguments\n",
    "from transformers.sagemaker import SageMakerTrainer as Trainer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0e73d",
   "metadata": {},
   "source": [
    "Now we only need to define the distribution parameter in the Hugging Face Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624809da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "hyperparameters={\n",
    "    \"epochs\": 1,                            # number of training epochs\n",
    "    \"train_batch_size\": 32,                 # training batch size\n",
    "    \"eval_batch_size\":64,                   #validation batch size\n",
    "    \"model_name\":\"distilbert-base-uncased\",  # name of pretrained model\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed model parallel\n",
    "mpi_options = {\n",
    "    \"enabled\" : True,\n",
    "    \"processes_per_host\" : 8\n",
    "}\n",
    "\n",
    "smp_options = {\n",
    "    \"enabled\":True,\n",
    "    \"parameters\": {\n",
    "        \"microbatches\": 4,\n",
    "        \"placement_strategy\": \"spread\",\n",
    "        \"pipeline\": \"interleaved\",\n",
    "        \"optimize\": \"speed\",\n",
    "        \"partitions\": 4,\n",
    "        \"ddp\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "distribution={\n",
    "    \"smdistributed\": {\"modelparallel\": smp_options},\n",
    "    \"mpi\": mpi_options\n",
    "}\n",
    "\n",
    "#Creating an estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",                 # fine-tuning script to use in training job\n",
    "    source_dir=\"./\",                 # directory where fine-tuning script is stored\n",
    "    instance_type=\"ml.p3dn.24xlarge\", # instance type\n",
    "    instance_count=2,                       # number of instances\n",
    "    role=role,                              # IAM role used in training job to acccess AWS resources (S3)\n",
    "    transformers_version=\"4.6\",             # Transformers version\n",
    "    pytorch_version=\"1.7\",                  # PyTorch version\n",
    "    py_version=\"py36\",                      # Python version\n",
    "    hyperparameters=hyperparameters         # hyperparameters to use in training job\n",
    "    distribution = distribution\n",
    ")\n",
    "\n",
    "huggingface_estimator.fit({\"train\": training_input_path, \"test\": test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd2fdf",
   "metadata": {},
   "source": [
    "# Deploying the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85482ce6",
   "metadata": {},
   "source": [
    "Deploy the fintuned model by calling deploy() on your estimator. then try to predict on some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(initial_instance_count=1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142ff125",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_input = {\"inputs\": \"put your sentence here to test the hate speech detection model\"}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e33a7e",
   "metadata": {},
   "source": [
    "Delete the endpoint and save costs :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1652ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
