{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76fc6348",
   "metadata": {},
   "source": [
    "# Run training on Amazon Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d1432",
   "metadata": {},
   "source": [
    "The purpose of this notebook is a code example of how to train a text classifier for content moderation in AWS environment in a non ditributed manner for the first time, then showcase how to extend to a distributed fine-tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4fb5c",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cfad17",
   "metadata": {},
   "source": [
    "As I am using Sagemaker notebooks, I am already using a kernel with Pytorch installed. So, all I have to do is to install sagemaker, transformers and datasets to download the dataset from Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab75b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeedf8f7",
   "metadata": {},
   "source": [
    "# Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518900e1",
   "metadata": {},
   "source": [
    "We need a sagemaker session. The session will handle the creation of a default bucket to store data, models, and logs.\n",
    "\n",
    "As I am working with a sagemaker notebook, I can use the .get_execution_role() function to access the role. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9668896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5836c77",
   "metadata": {},
   "source": [
    "# Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d3f988",
   "metadata": {},
   "source": [
    "I chose the tweets_hate_speech_detection dataset from Hugging Face hub to work on, as it is very close to a content moderation task.\n",
    "The labels for this dataset is either 0 (no hate speech) or 1 (hate speech), so it is not a multilabel text classification problem but the code stills the same for binary or multilabel classifiaction on AWS (except a little detail in the training script when dowloading the model with .from_pretrained : for a mulilabel class you should add the num_labels parameters and 2 dictionnaries mapping labels to ids and ids to labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d942ff7",
   "metadata": {},
   "source": [
    "I split the data into train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210edc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f8e703002a42b69093dac80f9e8b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1393.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933c9425c31b4e80afd0e8644cfac354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=827.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset tweets_hate_speech_detection/default (download: 2.96 MiB, generated: 3.04 MiB, post-processed: Unknown size, total: 6.00 MiB) to /home/ec2-user/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/b85ae55489e4a8c3531632a1b4e654546689115add2a15f8bbf0ecbd779ef3ff...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f59290ed6c42b49b40f906199c2206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1276746.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tweets_hate_speech_detection downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/b85ae55489e4a8c3531632a1b4e654546689115add2a15f8bbf0ecbd779ef3ff. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load dataset\n",
    "dataset = load_dataset(\"tweets_hate_speech_detection\")\n",
    "# split the data into train and test\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e5ce8",
   "metadata": {},
   "source": [
    "The preprocessing done here is just simple tokenization of the tweets.\n",
    "\n",
    "We do trunction and padding to the max length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f670c42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c267fd01b178445a88d4495a32b74feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=483.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6678df5a6d1b41f6afe98a46852e5fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd67bda40e0f4b17a65cabc5e8940087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de48a58c64154d369d0a89895aae6322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41888ad6726649618accac7cb8cc5eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65bee740df54dbcb6911aa8c04e187e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# create tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"tweet\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# tokenize train and test datasets\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa00c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename label with labels as the model is expecting the keyword\n",
    "tokenized_dataset =  tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# set dataset format for PyTorch\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74622e",
   "metadata": {},
   "source": [
    "# Upload data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344a657",
   "metadata": {},
   "source": [
    "Next, we upload the tokenized train data and test data to 2 different folders in the S3 bucket previously created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8214eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3_prefix = 'samples/datasets/tweetsspeech'\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to S3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "tokenized_dataset[\"train\"].save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to S3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "tokenized_dataset[\"test\"].save_to_disk(test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72107bc",
   "metadata": {},
   "source": [
    "# Fine-tuning with Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519491d",
   "metadata": {},
   "source": [
    "Now, we create the HuggingFace Estimator and we give it: \n",
    "* the fine-tuning script in the entry point parameter\n",
    "* the instance type of Sagemaker (each instance has a price, I tried to use the cheapest instance in the computing oriented category in my region)\n",
    "* try to benefit from the spot instances to save costs using the parameter use_spot_instances\n",
    "* finally we give the needed hyperparameters for our training script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fe11344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "hyperparameters={\n",
    "    \"epochs\": 1,                            # number of training epochs\n",
    "    \"train_batch_size\": 32,                 # training batch size\n",
    "    \"eval_batch_size\":64,                   #validation batch size\n",
    "    \"model_name\":\"distilbert-base-uncased\",  # name of pretrained model\n",
    "}\n",
    "\n",
    "#Creating an estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",                 # fine-tuning script to use in training job\n",
    "    source_dir=\"./\",                 # directory where fine-tuning script is stored\n",
    "    instance_type=\"ml.g4dn.2xlarge\", # instance type\n",
    "#     checkpoint_s3_uri=f's3://{sess.default_bucket()}/checkpoints',\n",
    "#     use_spot_instances=True,\n",
    "#     max_wait=3600,\n",
    "#     max_run=1000,\n",
    "    instance_count=1,                       # number of instances\n",
    "    role=role,                              # IAM role used in training job to acccess AWS resources (S3)\n",
    "    transformers_version=\"4.6\",             # Transformers version\n",
    "    pytorch_version=\"1.7\",                  # PyTorch version\n",
    "    py_version=\"py36\",                      # Python version\n",
    "    hyperparameters=hyperparameters         # hyperparameters to use in training job\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7c22e",
   "metadata": {},
   "source": [
    "We launch training with .fit function of the estimator object.\n",
    "\n",
    "I hit a quota limit as I am using a AWS account with no Sagemaker ressources associated in my region. So I had to ask to augment my quota, but I didn't get an answer and I should wait for the support team to get back to me.\n",
    "\n",
    "But if you are using your professional company account, surely you have some ressources in your region and you won't have this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b3d8a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.g4dn.2xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10040/187206430.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_input_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1466\u001b[0m         \"\"\"\n\u001b[1;32m   1467\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.g4dn.2xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit."
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({\"train\": training_input_path, \"test\": test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae49801",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "### Can you extend your example from 1) to a distributed setup?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0855dad",
   "metadata": {},
   "source": [
    "Sagemaker provides two strategies for distributed training:\n",
    "* data parallelism\n",
    "* model parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7219967",
   "metadata": {},
   "source": [
    "#### Data parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb217a",
   "metadata": {},
   "source": [
    "If you want to do data parallelism, ignore the two previous cells and run the next one instead.\n",
    "As we are using the Trainer API in our fineuning script, we only need to define the distribution parameter in the Hugging Face Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea7f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "hyperparameters={\n",
    "    \"epochs\": 1,                            # number of training epochs\n",
    "    \"train_batch_size\": 32,                 # training batch size\n",
    "    \"eval_batch_size\":64,                   #validation batch size\n",
    "    \"model_name\":\"distilbert-base-uncased\",  # name of pretrained model\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed data parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "#Creating an estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",                 # fine-tuning script to use in training job\n",
    "    source_dir=\"./\",                 # directory where fine-tuning script is stored\n",
    "    instance_type=\"ml.p3dn.24xlarge\", # instance type\n",
    "    instance_count=2,                       # number of instances\n",
    "    role=role,                              # IAM role used in training job to acccess AWS resources (S3)\n",
    "    transformers_version=\"4.6\",             # Transformers version\n",
    "    pytorch_version=\"1.7\",                  # PyTorch version\n",
    "    py_version=\"py36\",                      # Python version\n",
    "    hyperparameters=hyperparameters         # hyperparameters to use in training job\n",
    "    distribution = distribution\n",
    ")\n",
    "\n",
    "huggingface_estimator.fit({\"train\": training_input_path, \"test\": test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0169707",
   "metadata": {},
   "source": [
    "#### Model parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd24b47",
   "metadata": {},
   "source": [
    "To enable model-parallelism, a little change has to be done in the training script.\n",
    "Extend the Trainer API to a the SageMakerTrainer to use the model parallelism library by using these imports in the training script :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d24c277",
   "metadata": {},
   "source": [
    "```python\n",
    "from transformers.sagemaker import SageMakerTrainingArguments as TrainingArguments\n",
    "from transformers.sagemaker import SageMakerTrainer as Trainer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b99d4",
   "metadata": {},
   "source": [
    "Now we only need to define the distribution parameter in the Hugging Face Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697975f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "hyperparameters={\n",
    "    \"epochs\": 1,                            # number of training epochs\n",
    "    \"train_batch_size\": 32,                 # training batch size\n",
    "    \"eval_batch_size\":64,                   #validation batch size\n",
    "    \"model_name\":\"distilbert-base-uncased\",  # name of pretrained model\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed model parallel\n",
    "mpi_options = {\n",
    "    \"enabled\" : True,\n",
    "    \"processes_per_host\" : 8\n",
    "}\n",
    "\n",
    "smp_options = {\n",
    "    \"enabled\":True,\n",
    "    \"parameters\": {\n",
    "        \"microbatches\": 4,\n",
    "        \"placement_strategy\": \"spread\",\n",
    "        \"pipeline\": \"interleaved\",\n",
    "        \"optimize\": \"speed\",\n",
    "        \"partitions\": 4,\n",
    "        \"ddp\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "distribution={\n",
    "    \"smdistributed\": {\"modelparallel\": smp_options},\n",
    "    \"mpi\": mpi_options\n",
    "}\n",
    "\n",
    "#Creating an estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",                 # fine-tuning script to use in training job\n",
    "    source_dir=\"./\",                 # directory where fine-tuning script is stored\n",
    "    instance_type=\"ml.p3dn.24xlarge\", # instance type\n",
    "    instance_count=2,                       # number of instances\n",
    "    role=role,                              # IAM role used in training job to acccess AWS resources (S3)\n",
    "    transformers_version=\"4.6\",             # Transformers version\n",
    "    pytorch_version=\"1.7\",                  # PyTorch version\n",
    "    py_version=\"py36\",                      # Python version\n",
    "    hyperparameters=hyperparameters         # hyperparameters to use in training job\n",
    "    distribution = distribution\n",
    ")\n",
    "\n",
    "huggingface_estimator.fit({\"train\": training_input_path, \"test\": test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd1c82",
   "metadata": {},
   "source": [
    "# Deploying the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92586bf0",
   "metadata": {},
   "source": [
    "Deploy the fintuned model by calling deploy() on your estimator. then try to predict on some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d04a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(initial_instance_count=1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8090352",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_input = {\"inputs\": \"put your sentence here to test the hate speech detection model\"}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34bcaa",
   "metadata": {},
   "source": [
    "Delete the endpoint and save costs :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e53dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
